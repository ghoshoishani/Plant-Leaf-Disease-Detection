{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43eef921",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10501 images belonging to 4 classes.\n",
      "Found 2623 images belonging to 4 classes.\n",
      "Epoch 1/10\n",
      "328/328 [==============================] - 3391s 10s/step - loss: 0.1533 - accuracy: 0.9528 - val_loss: 5.3804 - val_accuracy: 0.6817\n",
      "Epoch 2/10\n",
      "328/328 [==============================] - 3213s 10s/step - loss: 0.0610 - accuracy: 0.9800 - val_loss: 9.6822 - val_accuracy: 0.6177\n",
      "Epoch 3/10\n",
      "328/328 [==============================] - 4533s 12s/step - loss: 0.0715 - accuracy: 0.9764 - val_loss: 10.5193 - val_accuracy: 0.5448\n",
      "Epoch 4/10\n",
      "328/328 [==============================] - 3525s 11s/step - loss: 0.0773 - accuracy: 0.9746 - val_loss: 4.2306 - val_accuracy: 0.7932\n",
      "Epoch 5/10\n",
      "328/328 [==============================] - 4110s 13s/step - loss: 0.0407 - accuracy: 0.9858 - val_loss: 3.5972 - val_accuracy: 0.6462\n",
      "Epoch 6/10\n",
      "328/328 [==============================] - 3279s 10s/step - loss: 0.0882 - accuracy: 0.9721 - val_loss: 5.8597 - val_accuracy: 0.7404\n",
      "Epoch 7/10\n",
      "328/328 [==============================] - 3190s 10s/step - loss: 0.0430 - accuracy: 0.9852 - val_loss: 3.8511 - val_accuracy: 0.7245\n",
      "Epoch 8/10\n",
      "328/328 [==============================] - 3329s 10s/step - loss: 0.0412 - accuracy: 0.9877 - val_loss: 0.1707 - val_accuracy: 0.9792\n",
      "Epoch 9/10\n",
      "328/328 [==============================] - 3234s 10s/step - loss: 0.0337 - accuracy: 0.9878 - val_loss: 0.9201 - val_accuracy: 0.8939\n",
      "Epoch 10/10\n",
      "328/328 [==============================] - 3219s 10s/step - loss: 0.0464 - accuracy: 0.9874 - val_loss: 6.9301 - val_accuracy: 0.5675\n",
      "Category: Apple\n",
      "Validation loss: 6.950658321380615\n",
      "Validation accuracy: 0.566145658493042\n",
      "Found 5244 images belonging to 4 classes.\n",
      "Found 1309 images belonging to 4 classes.\n",
      "Epoch 1/10\n",
      "163/163 [==============================] - 1680s 10s/step - loss: 0.2052 - accuracy: 0.9198 - val_loss: 0.5120 - val_accuracy: 0.8750\n",
      "Epoch 2/10\n",
      "163/163 [==============================] - 1585s 10s/step - loss: 0.1645 - accuracy: 0.9396 - val_loss: 1.1489 - val_accuracy: 0.8562\n",
      "Epoch 3/10\n",
      "163/163 [==============================] - 1608s 10s/step - loss: 0.1348 - accuracy: 0.9470 - val_loss: 1.6267 - val_accuracy: 0.8539\n",
      "Epoch 4/10\n",
      "163/163 [==============================] - 1588s 10s/step - loss: 0.1283 - accuracy: 0.9497 - val_loss: 0.7734 - val_accuracy: 0.8492\n",
      "Epoch 5/10\n",
      "163/163 [==============================] - 1588s 10s/step - loss: 0.1188 - accuracy: 0.9509 - val_loss: 0.7397 - val_accuracy: 0.8227\n",
      "Epoch 6/10\n",
      "163/163 [==============================] - 1591s 10s/step - loss: 0.1118 - accuracy: 0.9574 - val_loss: 1.5382 - val_accuracy: 0.6539\n",
      "Epoch 7/10\n",
      "163/163 [==============================] - 1578s 10s/step - loss: 0.0967 - accuracy: 0.9637 - val_loss: 0.5119 - val_accuracy: 0.8359\n",
      "Epoch 8/10\n",
      "163/163 [==============================] - 1583s 10s/step - loss: 0.0911 - accuracy: 0.9647 - val_loss: 2.7107 - val_accuracy: 0.3734\n",
      "Epoch 9/10\n",
      "163/163 [==============================] - 1555s 10s/step - loss: 0.0952 - accuracy: 0.9662 - val_loss: 5.6620 - val_accuracy: 0.2914\n",
      "Epoch 10/10\n",
      "163/163 [==============================] - 1569s 10s/step - loss: 0.0843 - accuracy: 0.9691 - val_loss: 3.9323 - val_accuracy: 0.4867\n",
      "Category: Citrus\n",
      "Validation loss: 3.946719169616699\n",
      "Validation accuracy: 0.48586708307266235\n",
      "Found 7589 images belonging to 3 classes.\n",
      "Found 1896 images belonging to 3 classes.\n",
      "Epoch 1/10\n",
      "237/237 [==============================] - 2416s 10s/step - loss: 0.1350 - accuracy: 0.9537 - val_loss: 13.6947 - val_accuracy: 0.3686\n",
      "Epoch 2/10\n",
      "237/237 [==============================] - 2359s 10s/step - loss: 0.0540 - accuracy: 0.9791 - val_loss: 27.3063 - val_accuracy: 0.2558\n",
      "Epoch 3/10\n",
      "237/237 [==============================] - 2364s 10s/step - loss: 0.0470 - accuracy: 0.9839 - val_loss: 12.3311 - val_accuracy: 0.5535\n",
      "Epoch 4/10\n",
      "237/237 [==============================] - 2355s 10s/step - loss: 0.0481 - accuracy: 0.9837 - val_loss: 5.5558 - val_accuracy: 0.6314\n",
      "Epoch 5/10\n",
      "237/237 [==============================] - 2365s 10s/step - loss: 0.0431 - accuracy: 0.9836 - val_loss: 11.8537 - val_accuracy: 0.3136\n",
      "Epoch 6/10\n",
      "237/237 [==============================] - 2340s 10s/step - loss: 0.0315 - accuracy: 0.9870 - val_loss: 1.1975 - val_accuracy: 0.8602\n",
      "Epoch 7/10\n",
      "237/237 [==============================] - 2351s 10s/step - loss: 0.0450 - accuracy: 0.9831 - val_loss: 6.7840 - val_accuracy: 0.6647\n",
      "Epoch 8/10\n",
      "237/237 [==============================] - 2417s 10s/step - loss: 0.0383 - accuracy: 0.9866 - val_loss: 5.0647 - val_accuracy: 0.6769\n",
      "Epoch 9/10\n",
      "237/237 [==============================] - 2376s 10s/step - loss: 0.0544 - accuracy: 0.9803 - val_loss: 22.8417 - val_accuracy: 0.2569\n",
      "Epoch 10/10\n",
      "237/237 [==============================] - 2316s 10s/step - loss: 0.0442 - accuracy: 0.9835 - val_loss: 17.6775 - val_accuracy: 0.2881\n",
      "Category: Potato\n",
      "Validation loss: 17.62537384033203\n",
      "Validation accuracy: 0.28955695033073425\n",
      "Found 4918 images belonging to 2 classes.\n",
      "Found 1228 images belonging to 2 classes.\n",
      "Epoch 1/10\n",
      "153/153 [==============================] - 1621s 10s/step - loss: 0.0269 - accuracy: 0.9926 - val_loss: 0.0384 - val_accuracy: 0.9926\n",
      "Epoch 2/10\n",
      "153/153 [==============================] - 1520s 10s/step - loss: 0.0080 - accuracy: 0.9973 - val_loss: 0.2461 - val_accuracy: 0.9408\n",
      "Epoch 3/10\n",
      "153/153 [==============================] - 1526s 10s/step - loss: 0.0028 - accuracy: 0.9988 - val_loss: 19.1035 - val_accuracy: 0.5148\n",
      "Epoch 4/10\n",
      "153/153 [==============================] - 1499s 10s/step - loss: 0.0088 - accuracy: 0.9975 - val_loss: 24.6081 - val_accuracy: 0.4934\n",
      "Epoch 5/10\n",
      "153/153 [==============================] - 1475s 10s/step - loss: 0.0045 - accuracy: 0.9977 - val_loss: 4.5131 - val_accuracy: 0.7878\n",
      "Epoch 6/10\n",
      "153/153 [==============================] - 1504s 10s/step - loss: 0.0020 - accuracy: 0.9992 - val_loss: 2.0543 - val_accuracy: 0.8997\n",
      "Epoch 7/10\n",
      "153/153 [==============================] - 1475s 10s/step - loss: 0.0033 - accuracy: 0.9984 - val_loss: 2.4639 - val_accuracy: 0.8076\n",
      "Epoch 8/10\n",
      "153/153 [==============================] - 1515s 10s/step - loss: 0.0048 - accuracy: 0.9988 - val_loss: 0.1499 - val_accuracy: 0.9753\n",
      "Epoch 9/10\n",
      "153/153 [==============================] - 1495s 10s/step - loss: 0.0058 - accuracy: 0.9980 - val_loss: 35.7354 - val_accuracy: 0.4589\n",
      "Epoch 10/10\n",
      "153/153 [==============================] - 1501s 10s/step - loss: 0.0080 - accuracy: 0.9982 - val_loss: 0.1413 - val_accuracy: 0.9844\n",
      "Category: Strawberry\n",
      "Validation loss: 0.1399315893650055\n",
      "Validation accuracy: 0.9845277070999146\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Set the path to the dataset directory\n",
    "dataset_dir = r\"C:/Users/Lenovo/Downloads/archive (7)/PlantDiseasesDataset\"\n",
    "\n",
    "# Set the desired image dimensions\n",
    "image_height, image_width = 224, 224\n",
    "\n",
    "# Set the batch size and number of epochs\n",
    "batch_size = 32\n",
    "epochs = 10\n",
    "\n",
    "# Set the number of classes\n",
    "num_classes = [4, 4, 3, 2]\n",
    "\n",
    "# Function to generate the path to each category's train and validation folders\n",
    "def get_category_path(category):\n",
    "    return os.path.join(dataset_dir, category, \"train\")\n",
    "\n",
    "# Load the MobileNetV2 model without the top classification layer\n",
    "base_model = MobileNetV2(\n",
    "    include_top=False,\n",
    "    input_shape=(image_height, image_width, 3),\n",
    "    weights='imagenet'\n",
    ")\n",
    "\n",
    "# Add a global average pooling layer and a dense output layer\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "\n",
    "# Loop through each category and train a separate model\n",
    "for i, category in enumerate(os.listdir(dataset_dir)):\n",
    "    category_path = get_category_path(category)\n",
    "\n",
    "    if not os.path.isdir(category_path):\n",
    "        continue\n",
    "\n",
    "    train_generator = ImageDataGenerator(\n",
    "        rescale=1.0 / 255,\n",
    "        rotation_range=20,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        validation_split=0.2\n",
    "    ).flow_from_directory(\n",
    "        category_path,\n",
    "        target_size=(image_height, image_width),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical',\n",
    "        subset='training'\n",
    "    )\n",
    "\n",
    "    validation_generator = ImageDataGenerator(\n",
    "        rescale=1.0 / 255,\n",
    "        validation_split=0.2\n",
    "    ).flow_from_directory(\n",
    "        category_path,\n",
    "        target_size=(image_height, image_width),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical',\n",
    "        subset='validation'\n",
    "    )\n",
    "\n",
    "    output = Dense(num_classes[i], activation='softmax')(x)\n",
    "    model = Model(inputs=base_model.input, outputs=output)\n",
    "\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    model.fit(\n",
    "        train_generator,\n",
    "        steps_per_epoch=train_generator.samples // batch_size,\n",
    "        epochs=epochs,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=validation_generator.samples // batch_size\n",
    "    )\n",
    "\n",
    "    # Evaluate the model on the validation set\n",
    "    score = model.evaluate(validation_generator, verbose=0)\n",
    "    print(f\"Category: {category}\")\n",
    "    print(\"Validation loss:\", score[0])\n",
    "    print(\"Validation accuracy:\", score[1])\n",
    "\n",
    "    # Save the trained model for each category\n",
    "    model.save(f\"{category}_plant_disease_detection_model.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a7bef1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
